""" Script to preprocess the captions in the training and test data """

import json
import operator
import pickle
from random import shuffle
import os
import numpy as np
import sys

# Creates the vocabulary given all the sentences (captions) in the training set
# Params : list of (sentences,video_id) in the training set
# Returns : dictionary of 10000 most frequent words with few utility tokens (<UNK>, <START>, <END>, <PAD>)
def create_vocab(sentences):
    word_count = {}
    for sent in sentences:
        temp = sent[0].split(' ')
        for word in temp:
            try:
                word_count[word] += 1
            except:
                word_count[word] = 1
    word_count = sorted(word_count.items(), key=operator.itemgetter(1), reverse = True)[:10000]
    shuffle(word_count)
    word_to_id = {}
    index = 0
    for pair in word_count:
        word_to_id[pair[0]] = index
        index += 1
    assert index == 10000
    word_to_id["<UNK>"] = 10000
    word_to_id["<START>"] = 10001
    word_to_id["<END>"] = 10002
    word_to_id["<PAD>"] = 10003
    return word_to_id

# Given the sentences and the vocabulary, generates a video to sentence (list of ids) mapping
# Params : Vocabulary (generated by previous function), Sentences (same format as above)
# Returns : Videos mapped to list of captions (list of ids)
def convert_to_id(word_to_id, sentences):
    vid_sent_map = {}
    for sent in sentences:
        temp = sent[0].split(' ')
        ind_sent = []
        ind_sent.append(word_to_id["<START>"])
        for word in temp:
            try:
                ind_sent.append(word_to_id[word])
            except:
                ind_sent.append(word_to_id["<UNK>"])
        ind_sent.append(word_to_id["<END>"])
        try:
            vid_sent_map[sent[1]].append(ind_sent)
        except:
            vid_sent_map[sent[1]] = [ind_sent]
    return vid_sent_map


def get_vid_ids(vid_path, audio_path):
    vid_files = []
    aud_files = []
    for file in os.listdir(vid_path):
        vid_files.append(file)
    for file in os.listdir(audio_path):
        aud_files.append(file)
    common_files = list(set(vid_files).intersection(aud_files))
    rm_indices = []
    for index in range(len(common_files)):
        data_audio = np.load('{}{}'.format(audio_path,common_files[index]))
        data_video = np.load('{}{}'.format(vid_path,common_files[index]))
        if data_audio.shape[0] == 0 or data_video.shape[0] == 0:
            rm_indices.append(index)
        common_files[index] = common_files[index].split('.')[0]
    elements = [common_files[ind] for ind in rm_indices]
    print(elements)
    print(len(elements))
    for element in elements:
        common_files.remove(element)
    return common_files

# Gets sentences by parsing the json file (data)
def get_sentences(data):
    sentences = [(data['sentences'][item]['caption'],data['sentences'][item]['video_id']) for item in range(len(data['sentences']))]
    return sentences

if __name__ == '__main__':
    with open(sys.argv[1]) as file:
        data = json.load(file)

    sentences = get_sentences(data)
    create_vocab(sentences)
